{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAT_DAML_DAY04_04ImageClassifier_RPS_TensorFlowGG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iFFqFsM0-Qrz",
        "aAeskrqbCy2q",
        "GXP2qqjIDeBc",
        "V1fWxhkUJ0Zx",
        "CU_yAzR7fLnG",
        "UvHYtSz4Liuu",
        "Mcb-3ZMnwcXh",
        "Vo5YGjrZwkRY",
        "qMrLWrTgNB7U",
        "P0URsjvuw4Jl",
        "uyD7kONFzxv7",
        "e_bxT7qrQbQU",
        "EuGX8JtqQe7Q",
        "aTt9r184685P",
        "2mmLdoxoL5a2",
        "-KJsnNJkMYHq"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonratNg/TEST/blob/main/DAT_DAML_DAY04_04ImageClassifier_RPS_TensorFlowGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfy8Wplim6SB"
      },
      "source": [
        "<table ><tr><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://web.facebook.com/DAT.KUSRC/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1dNBiKikzW1-osi6lleLOgSOKQ65IIfMC\" height=\"50px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://www.ku.ac.th/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ZfGOBmxAwg8SAhyseFziyinzxBGme78a\" height=\"80px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "<a href=\"https://www.tensorflow.org/\" target=\"_blank\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/1200px-TensorFlowLogo.svg.png\" height=\"80px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://mike.cpe.ku.ac.th/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1s6r3iG_Slpu_NSWqdt5zBp8Z9hV0-zh6\" height=\"50px\"></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fsj-W926IGN"
      },
      "source": [
        "---\n",
        "\n",
        "<center><h1><b>Image Classifier for Rock, Paper, and Scissors</b></h1></center>\n",
        "\n",
        "---\n",
        "*   Acknowledgement: Most parts of this tutorail were extracted from [Machine Learning, Zero to Hero](https://www.youtube.com/watch?v=u2TjZzNuly8&t=86s), Google TensorFlow 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htZXjfTZoZUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b0a69b-4c42-4445-9ab3-580a887c8d46"
      },
      "source": [
        "print('Image Classifier for Rock, Paper, and Scissors...')\n",
        "print('  Brought to you by K.Toto@MikeLab.Net')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Classifier for Rock, Paper, and Scissors...\n",
            "  Brought to you by K.Toto@MikeLab.Net\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFFqFsM0-Qrz"
      },
      "source": [
        "## 1. Recall the Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V_a2czd-Kvo"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1rJks1MAIu5a96fiJ-Ud8Vey5plS_8LCp)\n",
        "\n",
        "Remember this? From the first tutorial where we showed a scenario of rock, paper, and scissors, and discussed how difficult it might be to create an application that recognizes hands of different shapes, sizes, ethnicities, decorations, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJNcdza_EXY"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1e--Xp5ivQkS0ff-3BnqLPJ77LA65sCR6)\n",
        "\n",
        "We discussed how difficult it would be to write code to detect and classify these, even for something as simple as a rock, paper or scissors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2QNa6c-_qBR"
      },
      "source": [
        "<center><img src=https://drive.google.com/uc?id=1btOZmXxqv4OcIHc5fVHoImRBalyZC93k></center>\n",
        "\n",
        "But since then, we have looked into machine learning, and we have seen how to build neural networks, first to detect <font color=ff00ff>**patterns**</font> in raw pixels to classify them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lld-sNFfxFIH"
      },
      "source": [
        "<center><img src=https://drive.google.com/uc?id=1vtuQNe4b5maIbbMT8rYnK0SU2tVaawxc></center>\n",
        "\n",
        "And then to detect <font color=ff00ff>**features**</font> using <font color=ffff00>convolutions</font> (i.e., to have a <font color=ffff00>*convolution neural network*</font> <font color=00ffff>`trained`</font> <font color=00ff00>*to spot the particular features*</font> that make up an item like the [soles](https://www.google.com/search?q=sole+of+a+shoe) or the [silhouette](https://www.google.com/search?q=silhouette+of+a+shoe) of a shoe.\n",
        "\n",
        "Here we will see how to create a neural network that is trained on data of rock, paper, and scissors, to detect and spot them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAeskrqbCy2q"
      },
      "source": [
        "## 2. The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKrD-UPNC8V0"
      },
      "source": [
        "There is a dataset here that has several hundred images of rock, paper, and scissors poses. We will train a neural network with this data.\n",
        "\n",
        "First of all, we have to download the zip files containing the data. The code to do that is as follows. One file has the training set, the other has a testing and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52H-cMpkCzZ0"
      },
      "source": [
        "# map my GG drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')\n",
        "\n",
        "## --> before open the # sign, look at the destination directory at the end \n",
        "##     of these wget commands!!\n",
        "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip -O '/tmp/rps.zip'\n",
        "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip -O '/tmp/rps-test-set.zip'\n",
        "\n",
        "#print('Google Drive connected..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXP2qqjIDeBc"
      },
      "source": [
        "### Exploring the data\n",
        "In Python, we can unzip a file with the zip file library, and we unzip them to a temp directory like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlBokyQ8DxyL"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "## set your destination working (data path) directory here\n",
        "#WORK_DIR = '/content/drive/My Drive/00_DAT/'\n",
        "WORK_DIR = '/tmp'\n",
        "\n",
        "def extractFile(WORK_DIR): \n",
        "  print('Extracring all zip files to ' + WORK_DIR + '/tmp') \n",
        "  local_zip = WORK_DIR + '/rps.zip'\n",
        "  zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "  zip_ref.extractall(WORK_DIR + '/tmp')\n",
        "  zip_ref.close()\n",
        "\n",
        "  local_zip = WORK_DIR + '/rps-test-set.zip'\n",
        "  zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "  zip_ref.extractall(WORK_DIR + '/tmp')\n",
        "  zip_ref.close()\n",
        "  print('WARNING: Do not forget to disable me extractFile() after first use (call)!!')\n",
        "\n",
        "## The following line shoule be first run, only ONCE\n",
        "#extractFile(WORK_DIR)\n",
        "\n",
        "print('zipfile imported..')\n",
        "print('\\t>>Warning: Bofore proceeding to the next code block,\\n\\t  do not forget to configure your WORK_DIR //data// path..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76KGTPe3IwoV"
      },
      "source": [
        "This will create folders with sub-folders of each of our categories. When training in TensorFlow using an <font color=ff00ff>image data generator</font>, it will automatically label the images based on the name of their parent directory. So, we here don't need to create labels for the images.\n",
        "\n",
        "<center><img src=https://drive.google.com/uc?id=1cl4LKGFi3WY70RC0XJ4uNoN_l-POnhdz></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJMfaj4SoH--"
      },
      "source": [
        "import os\n",
        "\n",
        "rock_dir = os.path.join(WORK_DIR + '/tmp/rps/rock')\n",
        "paper_dir = os.path.join(WORK_DIR + '/tmp/rps/paper')\n",
        "scissors_dir = os.path.join(WORK_DIR +'/tmp/rps/scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
        "\n",
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:5])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:5])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:5])\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9dLel9N9DS"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 10\n",
        "nbPix = 1\n",
        "\n",
        "next_rock = [os.path.join(rock_dir, fname) for fname in rock_files[pic_index-nbPix:pic_index]]\n",
        "next_paper = [os.path.join(paper_dir, fname) for fname in paper_files[pic_index-nbPix:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname) for fname in scissors_files[pic_index-nbPix:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rock + next_paper + next_scissors):\n",
        "  print(img_path)\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()\n",
        "\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1fWxhkUJ0Zx"
      },
      "source": [
        "###Preparing the training data\n",
        "So, we will achieve that with this code. This creates an image data generator that generates images for the training from the directory that they were downloaded. We can set up something called a <font color=ff00ff>training generator</font>, which as its name suggests, creates traing data from that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6tAit-iKfLH"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = WORK_DIR + '/tmp/rps'\n",
        "training_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (150, 150),\n",
        "    class_mode = 'categorical'\n",
        ")\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU_yAzR7fLnG"
      },
      "source": [
        "---\n",
        "###Advanced Topic: Image augmentation test script\n",
        "\n",
        "You can <font color=ff00ff>script</font> this code block, first, and go back here when you do your exercise later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbI7BRjMe3V1"
      },
      "source": [
        "# DEBUG: image augmentation test using ImageDataGenerator\n",
        "# Reference: https://www.kdnuggets.com/2020/02/easy-image-dataset-augmentation-tensorflow.html\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = WORK_DIR + '/tmp/rps'\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      #vertical_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (150, 150),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'categorical'\n",
        ")\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si1koV98fVpr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plotImages(images_arr):\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(20,20))\n",
        "  axes = axes.flatten()\n",
        "  for img, ax in zip( images_arr, axes):\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuxnCBRlf9i_"
      },
      "source": [
        "augmented_images = [train_generator[0][0][0] for i in range(3)]\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvHYtSz4Liuu"
      },
      "source": [
        "###Preparing the testing data\n",
        "We can do exactly the same for the test set with this code. And later, when we see the ```model.fit(...)```, we will see that we passed these in as the training and validation parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH4-bM8XLn-i"
      },
      "source": [
        "VALIDATION_DIR = WORK_DIR + '/tmp/rps-test-set'\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size = (150, 150),\n",
        "    class_mode = 'categorical'\n",
        ")\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcb-3ZMnwcXh"
      },
      "source": [
        "## 3. Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBNlCAnjMnsA"
      },
      "source": [
        "Now let's look at our neuron network definition. This is very like what we have seen from the last tutorial, but just with more layers. \n",
        "\n",
        "One reason is that the rock, paper, and scissors' images are <font color=ff00ff>more complicated</font> than the grayscale clothing we saw previously, and another is that they are <font color=ff00ff>bigger</font>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW7eTksRNPux"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  ## Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "  #### This is the first convolution layer\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  #### The second convolution layer\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  #### The third convolution layer\n",
        "  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  #### The forth convolution layer\n",
        "  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  ##-------------------------------------------------------------------\n",
        "  ## Dense layer as we have seen before\n",
        "  ## Flatten the results to feed into a DNN\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  ## 512 neuron hidden layer\n",
        "  tf.keras.layers.Dense(512, activation='relu'),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "#--------\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "tz_BKK = pytz.timezone('Asia/Bangkok') \n",
        "datetime_BKK = datetime.now(tz_BKK)\n",
        "print(\"Current time:\", datetime_BKK.strftime(\"%H:%M:%S %d/%m/%y\"))\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDfNUJlHO2Mh"
      },
      "source": [
        "We can see that our input is now <font color=00ffff>150x150</font> pixel picture, with <font color=00ffff>3-byte</font> color depth. Our images are certainly bigger that they were before. And our output is a layer of the three neurons for the 3 classes of rock, paper, and scissors.\n",
        "\n",
        "The middle part of the code is very similar to what we saw previously, but just more of it. So, here we have four <font color=ff00ff>layers of convolutions</font>, each with <font color=ff00ff>MaxPooling</font>. \n",
        "\n",
        "Before feeding into a dense layer, the <font color=ff00ff>dropout</font> is a little trick to improve the efficiency of a neuron network by throwing away some of the neurons.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We will then <font color=ff00ff>**compile**</font> the neuron network model as before with this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRw0LYUpQOFX"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', \n",
        "              optimizer = 'rmsprop', \n",
        "              metrics = ['accuracy'])\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na_nmLhkQf_b"
      },
      "source": [
        "And then we can fit the data with the <font color=00ffff>```model.fit(...)```</font> call. Note that we don't have labels, as we are using the generator. it's inferring the labels from the parent directories of both the training and the validation datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo5YGjrZwkRY"
      },
      "source": [
        "## 4. Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTkY0iIb3j8u"
      },
      "source": [
        "<font color='fffff00'>**Note that the fitting process takes very long time!**</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oecTK-f1rqrw"
      },
      "source": [
        "# This process takes a long time, so think twice before running this code block\n",
        "history = model.fit_generator(train_generator, epochs=25, steps_per_epoch=20,\n",
        "                              validation_data = validation_generator, \n",
        "                              verbose = 1, validation_steps=3)\n",
        "print('-------')\n",
        "print('model fitting, DONE..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I41DxGYxjaN"
      },
      "source": [
        "\n",
        "So after finish, we can save the whole model to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FSu2cQPDAjB"
      },
      "source": [
        "#type(history.history)\n",
        "#history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMrLWrTgNB7U"
      },
      "source": [
        "#### Think twice before running this code fragment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28EUSRjVQ3LA"
      },
      "source": [
        "# Saving the model and result (aka. history) for later usage\n",
        "WORK_DIR = '/tmp'\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "####---------------------------------------------------------\n",
        "model.save(WORK_DIR + '/tmp/rps.h5')\n",
        "fout = open(WORK_DIR + '/tmp/history.txt', 'w')\n",
        "#### COMMENT the two above lines and rather use these lines\n",
        "#### if you prepare the training data with \n",
        "#### image augmentation technique (advanced topic)\n",
        "#model.save(WORK_DIR + '/tmp/rps_img_augmented.h5')\n",
        "#fout = open(WORK_DIR + '/tmp/history_img_augmentd.txt', 'w')\n",
        "####---------------------------------------------------------\n",
        "\n",
        "m = ','.join(str(i) for i in acc)\n",
        "fout.write(m+'\\n')\n",
        "m = ','.join(str(i) for i in val_acc)\n",
        "fout.write(m+'\\n')\n",
        "m = ','.join(str(i) for i in loss)\n",
        "fout.write(m+'\\n')\n",
        "m = ','.join(str(i) for i in val_loss)\n",
        "fout.write(m+'\\n')\n",
        "fout.close()\n",
        "\n",
        "print('Done..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0URsjvuw4Jl"
      },
      "source": [
        "## 5. Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrE8osJ2Zyvz"
      },
      "source": [
        "WORK_DIR = '/tmp'\n",
        "####---------------------------------------------------------\n",
        "fin = open(WORK_DIR + '/tmp/history.txt')\n",
        "#### COMMENT the two above lines and rather use these lines\n",
        "#### if you prepare the training data with \n",
        "#### image augmentation technique (advanced topic)\n",
        "#fin = open(WORK_DIR + '/tmp/history_img_augmentd.txt')\n",
        "####---------------------------------------------------------\n",
        "m = fin.read().splitlines()\n",
        "\n",
        "acc = [float(i) for i in m[0].split(',')]\n",
        "val_acc = [float(i) for i in m[1].split(',')]\n",
        "loss = [float(i) for i in m[2].split(',')]\n",
        "val_loss = [float(i) for i in m[3].split(',')]\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WNoOtZVGGJs"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyAWPIArxABq"
      },
      "source": [
        "After the model has been been fitted, we'll probably get accuracy about 100% on the training data quite quickly, but with the validation data getting to about 87% accuracy. \n",
        "\n",
        "The event is something called <font color=ffff00>**over-fitting**</font>, which happens when the model gets really good at spotting what it has seen before, but it is not so great at generalizing.\n",
        "\n",
        "Think about it this way, for example, if all our life, the only shoes that we had ever seen were hiking boots, we probably wouldn't recognize high heels as shoes. This can be said that we would be <font color=ffff00>over-fitting</font> <font color=ff00ff>ourselves</font>. \n",
        "\n",
        "There are a number of methods to avoid this, and one of them is called <font color=00ffff>image augmentation</font>. (*This [part](https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/) has been left for students to further explore.*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyD7kONFzxv7"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZABJp7T3VLCU"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "\n",
        "# Reload the exact same model, including its weights and the optimizer\n",
        "model = tf.keras.models.load_model(WORK_DIR + '/tmp/rps.h5')\n",
        "# Show the model architecture\n",
        "#model.summary()\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_bxT7qrQbQU"
      },
      "source": [
        "##Your exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### Exercise 1\n",
        "\n",
        "Try editing the convolutions. Change the number of convolutions from 32 to either 16 or 64. What impact does that have on accuracy and training time?\n",
        "\n",
        "#### Exercise 2\n",
        "\n",
        "Remove the final convolution. What impact does that have on accuracy or training time?\n",
        "\n",
        "#### Exercise 3\n",
        "\n",
        "Add more convolutions. What impact does that have?\n",
        "\n",
        "#### Exercise 4\n",
        "\n",
        "Remove all convolutions but the first. What impact does that have? Experiment with it.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CKuineP-6S_4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGX8JtqQe7Q"
      },
      "source": [
        "### Advance Topic I\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Go back to <font color=00ffff>Advanced Topic: Image augmentation test script</font> in section **2. The data**, and do a self-learning under the lecturer's supervision to re-plot the evaluation curves for comparison with the old ones. rorganize your code blocks, including with text blocks if it is necessary, below this line.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ANcVnUlcNPXJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY69es5mRkN3"
      },
      "source": [
        "# your exercise here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello, World\")\n",
        "# http://bit.ly/2lXXdw5"
      ],
      "metadata": {
        "id": "TIHZOymijEFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Topic II\n",
        "\n"
      ],
      "metadata": {
        "id": "aTt9r184685P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://developers.google.com/codelabs/tensorflow-5-compleximages#0\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zzJkBMlz7F9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Topic III\n",
        "\n"
      ],
      "metadata": {
        "id": "2mmLdoxoL5a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://developers.google.com/codelabs/tensorflow-6-largecnns#0\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nSfujFNpMdu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Playground\n"
      ],
      "metadata": {
        "id": "-KJsnNJkMYHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "2Gy7oc4tQvrl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}